This is Info file pm.info, produced by Makeinfo version 1.68 from the
input file bigpm.texi.


File: pm.info,  Node: WML/Card,  Next: WML/Deck,  Prev: WDDX,  Up: Module List

Perl extension for builiding WML Cards according to the browser being used.
***************************************************************************

NAME
====

   WML::Card - Perl extension for builiding WML Cards according to the
browser being used.

SYNOPSIS
========

   use WML::Card;

   my $options= [         ['Option 1', 'http://...'],         ['Option 2',
'http://...'], ];

   my $c = WML::Card->guess('index','Wap Site'); $c->link_list('indice',
undef,  0, $options,  $options); $c->print;

DESCRIPTION
===========

   This perl library simplifies the creation of  WML cards on the fly. It
produces the most suitable wml code for the browser requesting the card.
In this way the one building the cards does not have to worry about the
differences in how  each wap browser displays the wml code. In combination
wht WML::Deck it provides functionality to build WAP applications.

Methods
-------

$card = WML::Card->guess( $id, $title, [$user_agent] );
     This class method constructs a new WML::Card object.  The first
     argument defines the WML card's id and the second argument its title.
     The if the third argument is not defined, the value is obtained from
     $ENV{'HTTP_USER_AGENT'}.

$c->buttons($label, $type, $task, $href)
$c->table ($data,  $title, $offset, $pager, @headers)
$c->link_list($name, $listtitle, $offset, $pager, $data, $align)
$c->value_list($name, $listtitle, $offset,$pager,$data)
     The variable $data is an array reference like: my $menu_items= [
       ['Option 1', 'http://...'],         ['Option 2', 'http://...'], ];

     The variable $pager is the number of items wanted to be displayed in
     each card.

$c->print
$c->info($content)
$c->img($file, $alt)
$c->input($label, $text, $name, $format, $type, $size, $target, $arg);
$c->link($target, $text);
$c->br
AUTHOR
======

   Mariana Alvaro			mariana@alvaro.com.ar

   Copyright 2000 Mariana Alvaro. All rights reserved.  This library is
free software; you can redistribute it and/or modify it under the same
terms as Perl itself.

SEE ALSO
========

   WML::Deck


File: pm.info,  Node: WML/Deck,  Next: WWW/BBSWatch,  Prev: WML/Card,  Up: Module List

Perl extension for builiding WML Decks.
***************************************

NAME
====

   WML::Deck - Perl extension for builiding WML Decks.

SYNOPSIS
========

   use WML::Card;

   use WML::Deck;

   my @cards;

   my $options= [         ['Option 1', 'http://...'],         ['Option 2',
'http://...'],

   ];

   my $c = WML::Card->guess('index','Wap Site'); $c->link_list('indice',
undef,  0, $options,  $options); push @cards, $c;

   # Build the deck my $wml = WML::Deck->new(@cards); $wml->return_cgi;

DESCRIPTION
===========

   This perl library simplifies the creation of  WML decks on the fly. In
combination with WML::Card it provides functionality to build WML code for
cards and decks.

Methods
-------

$wml = WML::Deck->new(@cards);
     This class method constructs a new WML::Deck object.  The first
     argument is an array of WML::Card objects.

$wml->cache($n);
     This class methos specifies the max-age argument for Cache-Control

$wml->return_cgi;
     This method prints wml code and HTTP headers for the deck.

AUTHOR
======

   Mariana Alvaro			mariana@alvaro.com.ar

   Copyright 2000 Mariana Alvaro. All rights reserved.  This library is
free software; you can redistribute it and/or modify it under the same
terms as Perl itself.

SEE ALSO
========

   WML::Card


File: pm.info,  Node: WWW/BBSWatch,  Next: WWW/Babelfish,  Prev: WML/Deck,  Up: Module List

Send, via email, messages posted to a WWW bulletin board
********************************************************

NAME
====

   WWW::BBSWatch - Send, via email, messages posted to a WWW bulletin board

SYNOPSIS
========

     use WWW::BBSWatch; # should really be a subclass

     sub WWW::BBSWatch::article_list { # generates warning (rightly so)
       my $self = shift;
       my $content = shift;
       return ($$content =~ m%<A HREF="($self->{bbs_url}\?read=\d*)%gs);
     }

     BBSWatch->new(-MAIL => 'me',
       -BBS_URL => 'http://www.foo.org/cgi-bin/bbs.pl')->retrieve;

   See better, working examples below.

DESCRIPTION
===========

   There are many interesting discussions that take place on World Wide Web
Bulletin Boards, but I do not have the patience to browse to each article.
I can process email and newsgroups many times faster than a WWW bulletin
board because of the lag inherent in the web. Instead of ignoring this
wealth of information, *WWW::BBSWatch* was created. It will monitor a
World Wide Web Bulletin Board and email new postings to you. The email
headers are as correct as possible, including reasonable From, Subject,
Date, *Message-Id* and References entries.

   This module requires *LWP::UserAgent* and *MIME::Lite*.

INTERFACE
=========

$b = WWW::BBSWatch->new
     Arguments are:

     `-BBS_URL': The URL of the bulletin board's index page. This field is
     required.

     `-MAIL': The email address to send mail to

     `-DB': Basename of the database that keeps track of visited articles

     `-WARN_TIMEOUT': Number of seconds before warning message is sent
     proclaiming inability to contact BBS_URL page. Default is 10,800 (3
     hours).

     `-MAX_ARTICLES': Maximum number of articles to send in one batch.
     Default is essentially all articles.

     `-VERBOSE': Controls the amount of informative output. Useful values
     are 0, 1, 2. Default is 0 (completely silent).

$b->retrieve([$catchup])
     This method emails new bulletin board messages. If the optional
     parameter catchup is true, messages will be marked as read without
     being emailed. Nothing useful will happen unless the `article_list'
     method is defined to return the list of articles from the BBS's index
     page.

     *WWW::BBSWatch* uses the *LWP::UserAgent* module to retrieve the
     index and articles. It honors firewall proxies by calling the
     `LWP::UserAgent::env_proxy' method. So if you are behind a firewall,
     define the environment variable http_proxy and your firewall will be
     handled correctly.

USER-REFINABLE METHODS
======================

$b->article_list($content_ref)
     Method that returns a list of complete URLs for the articles on the
     bulletin board. It is passed a reference to the contents of the
     bbs_url page. The base version does not do anything.

$b->get_header_info($content_ref)
     Method that returns the header info for the message. It is passed a
     scalar reference to the entire HTML for the message. The method
     should return a list of

          * the poster's email address
          * the poster's name
          * the article's subject
          * the article's timestamp
          * any response-to message URL

     Any values in the return list can be undef, but the more info
     returned, the more useful the email headers will be. The base version
     of the method doesn't do anything.

$b->process_article($content_ref)
     Method that is used to process the article before it is mailed. It is
     passed a reference to the contents of the article. It should return a
     list of the MIME type of the article and a reference to the contents
     of the article. For example, you could refine this method to run the
     article through *HTML::FormatText* so that text messages are sent
     instead of HTML ones. The default method returns the list of
     `text/html' and its argument untouched.

PRACTICAL EXAMPLES
==================

   Here are examples of how I personally use *WWW::BBSWatch*. A useful
assumption is that WWW bulletin boards are programmatically generated so
the HTML of the articles tends to be very regular and predictable. This
allows regular expression matching when pulling header info or processing
articles instead of having to use HTML::Parser or HTML::TreeBuilder.

Monitoring the Perl Modules BBS
===============================

     package MyBBSWatch;

     use strict;
     use vars qw/@ISA/;
     use WWW::BBSWatch;
     @ISA = qw/WWW::BBSWatch/;

     sub get_header_info {
       my $self = shift;
       my $content_ref = shift;

     my ($name, $addr) =
       $$content_ref =~ m%<b>From</b>:\s*(.*) .*;<A HREF="mailto:(.*)">%m;
     $name =~ s/^"|"$//g;
     my ($subj) = $$content_ref =~ m%<H1>(.*)</H1>%m;
     my ($date) = $$content_ref =~ m%^<LI><b>Date</b>: (.*)</LI>$%m;
     $subj = "$subj [MODULES]"; # add tag for easy mail filtering
     return ($addr, $name, $subj, $date);
       }

     sub article_list {
       my $self = shift;
       my $content_ref = shift;

     my $base = $self->_base();
     return map { "$base/$_" }
       ($$content_ref =~ m%<A NAME="\d*" HREF="(msg\d*.html)">%sg);
       }

     # The index page of the Perl Modules list changes every month.
     # For everyone's benefit define a new method to figure out the index URL.
     sub _base {
       my ($y, $m) = (localtime)[5,4];
       return sprintf("http://www.xray.mpe.mpg.de/mailing-lists/modules/%4d-%02d",
                      $y+1900, $m+1);
     }

     package main;

     my $b = MyBBSWatch->new(
       -MAIL      =>'tayers',
       -BBS_URL   =>MyBBSWatch->_base()."/index.html",
       -DB        =>'/home/users/tayers/perl/.modules',
                            );

     $b->retrieve();

Monitoring two BBS's run from the same engine
=============================================

     package TheOakBBSWatch;

     # To watch multiple bulletin boards using the same engine requires
     # defining only one subclass of WWW::BBSWatch since the bulletin board
     # engine will generate the various boards in the same general format

     use vars qw/@ISA/;
     use WWW::BBSWatch;
     @ISA = qw/WWW::BBSWatch/;

     sub get_header_info {
       my $self = shift;
       my $content_ref = shift;

     my ($name) = $$content_ref =~ m%Posted By: <BIG>(.*)</BIG>%m;
     $name =~ s/^"|"$//g; # strip double-quotes

     my ($addr) = $$content_ref =~ m%<A HREF="mailto:.*subject.*>(.*)</A>%m;

     my ($subj) = $$content_ref =~ m%<H1 ALIGN=CENTER>(.*)</H1>%m;
     $subj = "$subj $self->{tag}"; # add a tag for filtering mail

     my ($date) = $$content_ref =~ /.*Date: (.*)$/m;

     my ($parent) = $$content_ref =~ m%In Response To: <A HREF="([^"]*)"%m;

     return ($addr, $name, $subj, $date, $parent);
       }

     sub article_list {
       my $self = shift;
       my $content_ref = shift;
       return ($$content_ref =~ m%<A HREF="($self->{bbs_url}\?read=\d*)%gs);
     }

     # Send articles from these bulletin boards as plain text. Hack the
     # HTML::FormatText to print out the href link as well as the "title". (Is
     # there a proper way to do this?) Changing the behavior this way works in
     # practice because the interesting links in BBS messages (the links that
     # people include in their message) are almost always fully specified. In
     # general this won't work since most links in documents are relative so
     # you need to keep track of the base.
     # Redefine the function in a backhanded way to suppress the "Subroutine
     # redefined" warning.
     use HTML::TreeBuilder;
     use HTML::FormatText;
     {
       local $ = 0;
       *HTML::Formatter::a_start = sub {
         my ($self, $el) = @_;
         $self->out($el->attr('href')." - ");
         $self->{anchor}++;
         1;
       };
       *HTML::Formatter::img_start = sub {
         my ($self, $el) = @_;
         $self->out($el->attr('src')." - ".($el->attr('alt') || "[IMAGE]"));
       };
     }

     sub process_article {
       my $self = shift;
       my $content_ref = shift;

     $$content_ref =~ s%<H2 ALIGN=CENTER><A NAME="Responses">.*$%</BODY></HTML>%s;
     $$content_ref =~ s%<A NAME="PostResponse"></A>.*$%</BODY></HTML>%s;

     my $tree = HTML::TreeBuilder->new->parse($$content_ref);
     $tree->eof;
     my $text = HTML::FormatText->new(leftmargin=>0)->format($tree);
     $tree->delete;
     return ('text/plain', \$text);
       }

     package main;

     # Advertise the firewall
     $ENV{http_proxy} = 'http://httpproxy:411';

     # Grab the general list
     my $b = TheOakBBSWatch->new(
       -MAIL      =>'tayers',
       -BBS_URL   =>'http://theoak.com/cgi-bin/forum/general.pl',
       -DB        =>'/home/users/tayers/perl/general',
       -MAX_ARTICLES => 250,
       -VERBOSITY =>0,
                            );

     # Break OO design by using knowledge of the underlying data structure.
     # The correct way is to refine new() and pass in -TAG, but this is
     # SO MUCH easier. (Famous last words!) The 'tag' is used in the
     # get_header_info method.
     $b->{tag} = "[OAK-GEN]";

     $b->retrieve();

     # Grab the Tools list
     $b = TheOakBBSWatch->new(
       -MAIL      =>'tayers',
       -BBS_URL   =>'http://theoak.com/cgi-bin/tools1/tools1.pl',
       -DB        =>'/home/users/tayers/perl/tools',
       -MAX_ARTICLES => 250,
       -VERBOSITY =>0,
                            );

     $b->{tag} = "[OAK-TOOL]";

     $b->retrieve();

SEE ALSO
========

   *Note Perlre: (perl.info)perlre,. At least a passing knowledge of
regular expressions helps quite a bit.

AUTHOR
======

     This module was written by
     Tim Ayers (http://search.cpan.org/search?mode=author&query=tayers).

COPYRIGHT
=========

   Copyright (c) 2000 Tim R. Ayers.

   All rights reserved. This program is free software; you can
redistribute it and/or modify it under the same terms as Perl itself.


File: pm.info,  Node: WWW/Babelfish,  Next: WWW/Poll,  Prev: WWW/BBSWatch,  Up: Module List

Perl extension for translation via babelfish
********************************************

NAME
====

   WWW::Babelfish - Perl extension for translation via babelfish

SYNOPSIS
========

     use WWW::Babelfish;
     $obj = new WWW::Babelfish( 'agent' => 'Mozilla/8.0', 'proxy' => 'myproxy' );
     die( "Babelfish server unavailable\n" ) unless defined($obj);

     $french_text = $obj->translate( 'source' => 'English',
                                     'destination' => 'French',
                                     'text' => 'My hovercraft is full of eels',
     				  'delimiter' => "\n\t",
     				  'ofh' => \*STDOUT );
     die("Could not translate: " . $obj->error) unless defined($french_text);

     @languages = $obj->languages;

DESCRIPTION
===========

   Perl interface to the WWW babelfish translation server.

METHODS
=======

new
     Creates a new WWW::Babelfish object. It can take a named argument for
     user agent.

languages
     Returns a plain array of the languages available for translation.

translate
     Translates some text using Babelfish.

     Parameters:

          source:      Source language
          destination: Destination language
          text:        If this is a reference, translate interprets it as an
                       open filehandle to read from. Otherwise, it is treated
                       as a string to translate.
          delimiter:   Paragraph delimiter for the text; the default is "\n\n".
                       Note that this is a string, not a regexp.
          ofh:         Output filehandle; if provided, the translation will be
                       written to this filehandle.

     If no ofh parameter is given, translate will return the text;
     otherwise it will return 1. On failure it returns undef.

error
     Returns a (hopefully) meaningful error string.

NOTES
=====

   Babelfish translates 1000 characters at a time. This module tries to
break the source text into reasonable logical chunks of less than 1000
characters, feeds them to Babelfish and then reassembles them. Formatting
may get lost in the process.

AUTHOR
======

   Dan Urist, durist@world.std.com

SEE ALSO
========

   perl(1).


File: pm.info,  Node: WWW/Poll,  Next: WWW/Robot,  Prev: WWW/Babelfish,  Up: Module List

Perl extension to build web polls
*********************************

NAME
====

   Poll - Perl extension to build web polls

DESCRIPTION
===========

     Perl module to build and run web polls with built-in administrative capabilities.

SYNOPSIS
========

     use Poll;
     my $poll = new Poll;
     $poll->path('/system/path/to/data/directory');

   # Voting and returning poll results

     $poll->vote($ans_key);
     $html = $poll->get_html();
     print "Content-type: text/html\n\n";
     print $html;

   #- Create a new Poll -#

   	$poll->question('Should Trent Lott change his barber?');
$poll->add_answers( "Yes", "No", "Who's Trent Lott?", etc );
$poll->create();

USAGE
=====

     $poll->path($directory);

   Above system directory must me chmod'ed 666.  Also, it needs to contain
the files qid.dat & questions.dat as world writable.   The graphic to
create the default percentage graph also goes in this directory.

   #- Retrieving Poll Data -#

   	$html = $poll->get_html(<pollid>);

   Returns default html of specific poll results.  With no parameter the
script returns the latest poll.

   -OR-

   	$poll_id = $poll->get(<pollid>);

   This command retrieve the specified poll but returns the poll id rather
than html. Using this method the poll objects can be accessed for
customized formatting of output.  Example below:

     $poll_id = $poll->get(<pollid>);
     print $poll->question();
     foreach ($poll->rkeys) {
     	print $poll->answers->{$_}." = ".$poll->votes->{$_}."<BR>";
     }

   #- Voting on Latest Poll -#

     $poll->vote($ans_key);

   Takes hash key for appropriate $poll->answers.  Keys can be gotten via
$poll->akeys.  Example below:

   	foreach ($poll->akeys) { 		print "Answer =
".$poll->answers->{$_}."\n"; 		print "Key = ".$_."\n"; 	}

   #- Create a new Poll -#

     $poll->question('Should Trent Lott change his barber?');
     $poll->add_answers( "Yes", "No", "Who's Trent Lott?", etc );
     $poll->create();

   This is pretty straight-forward.  There can be an infinite amount of
answers for any giver question but be aware of how it may look when
outputted to html.  The create() command builds the appropriate poll files
in the $poll->path() directory.

   #- To get a hash array of all polls to date -#

     my %all_polls = $poll->list();
     foreach (keys %polls) {
     	print qq|<A HREF="$ENV{SCRIPT_NAME}?poll_id=$_">$all_polls{$_}</A><BR>|;
     }

   This would print out a list of polls with links that could be followed
to view the results of that poll.

DOCUMENTATION
=============

   Documentation and code examples for Poll.pm can be located at
http://www.straphanger.org/~mgammon/poll

   The code examples located at the above url handle both administrative
and standard polling routines. There are currently no manpages for this
module but I will be working on them and post a revision when available.

PREREQUISITES
=============

   Perl 5.004

   May work with earlier versions but hasn't been tested.   Feel free to
email me if you find it does.

AUTHOR
======

   Mike Gammon <mgammon@straphanger.org>

   perl(1).


File: pm.info,  Node: WWW/Robot,  Next: WWW/RobotRules,  Prev: WWW/Poll,  Up: Module List

configurable web traversal engine (for web robots & agents)
***********************************************************

NAME
====

   WWW::Robot - configurable web traversal engine (for web robots & agents)

SYNOPSIS
========

     use WWW::Robot;
     
     $robot = new WWW::Robot(
         'NAME'     => 'MyRobot',
         'VERSION'  => '1.000',
         'EMAIL'    => 'fred@foobar.com'
     );
     
     # ... configure the robot's operation ...
     
     $robot->run( 'http://www.foobar.com/' );

DESCRIPTION
===========

   This module implements a configurable web traversal engine, for a
*robot* or other web agent.  Given an initial web page (URL), the Robot
will get the contents of that page, and extract all links on the page,
adding them to a list of URLs to visit.

   Features of the Robot module include:

   * Follows the *Robot Exclusion Protocol*.

   * Supports the META element proposed extensions to the Protocol.

   * Implements many of the *Guidelines for Robot Writers*.

   * Configurable.

   * Builds on standard Perl 5 modules for WWW, HTTP, HTML, etc.

   A particular application (robot instance) has to configure the engine
using *hooks*, which are perl functions invoked by the Robot engine at
specific points in the control loop.

   The robot engine obeys the Robot Exclusion protocol, as well as a
proposed addition.  See `SEE ALSO' in this node for references to
documents describing the Robot Exclusion protocol and web robots.

QUESTIONS
=========

   This section contains a number of questions. I'm interested in hearing
what people think, and what you've done faced with similar questions.

   * What style of API is preferable for setting attributes? Maybe
     something like the following:

          $robot->verbose(1);
          $traversal = $robot->traversal();

     I.e. a method for setting and getting each attribute, depending on
     whether you passed an argument?

   * Should the robot module support a standard logging mechanism?  For
     example, an LOGFILE attribute, which is set to either a filename, or
     a filehandle reference.  This would need a useful file format.

   * Should the module also support an ERRLOG attribute, with all warnings
     and error messages sent there?

   * At the moment the robot will print warnings and error messages to
     stderr, as well as returning error status. Should this behaviour be
     configurable?  I.e. the ability to turn off warnings.

   The basic architecture of the Robot is as follows:

     Hook: restore-state
     Get Next URL
         Hook: invoke-on-all-url
         Hook: follow-url-test
         Hook: invoke-on-followed-url
         Get contents of URL
         Hook: invoke-on-contents
         Skip if not HTML
         Foreach link on page:
             Hook: invoke-on-link
             Hook: add-url-test
             Add link to robot's queue
     Continue? Hook: continue-test
     Hook: save-state
     Hook: generate-report

   Each of the hook procedures and functions is described below.  A robot
must provide a follow-url-test hook, and at least one of the following:

   * invoke-on-all-url

   * invoke-on-followed-url

   * invoke-on-contents

   * invoke-on-link

CONSTRUCTOR
===========

     $robot = new WWW::Robot( <attribute-value-pairs> );

   Create a new robot engine instance.  If the constructor fails for any
reason, a warning message will be printed, and undef will be returned.

   Having created a new robot, it should be configured using the methods
described below.  Certain attributes of the Robot can be set during
creation; they can be (re)set after creation, using the `setAttribute()'
method.

   The attributes of the Robot are described below, in the *Robot
Attributes* section.

METHODS
=======

run
---

     $robot->run( @url_list );

   Invokes the robot, initially traversing the root URLs provided in
`@url_list', and any which have been provided with the `addUrl()' method
before invoking run().  If you have not correctly configured the robot,
the method will return undef.

   The initial set of URLs can either be passed as arguments to the run()
method, or with the *addUrl()* method before you invoke run().  Each URL
can be specified either as a string, or as a URI::URL object.

   Before invoking this method, you should have provided at least some of
the hook functions.  See the example given in the EXAMPLES section below.

   By default the run() method will iterate until there are no more URLs
in the queue.  You can override this behavior by providing a continue-test
hook function, which checks for the termination conditions.  This
particular hook function, and use of hook functions in general, are
described below.

setAttribute
------------

     $robot->setAttribute( ... attribute-value-pairs ... );

   Change the value of one or more robot attributes.  Attributes are
identified using a string, and take scalar values.  For example, to
specify the name of your robot, you set the NAME attribute:

     $robot->setAttribute( 'NAME' => 'WebStud' );

   The supported attributes for the Robot module are listed below, in the
*ROBOT ATTRIBUTES* section.

getAttribute
------------

     $value = $robot->getAttribute( 'attribute-name' );

   Queries a Robot for the value of an attribute.  For example, to query
the version number of your robot, you would get the VERSION attribute:

     $version = $robot->getAttribute( 'VERSION' );

   The supported attributes for the Robot module are listed below, in the
*ROBOT ATTRIBUTES* section.

getAgent
--------

     $agent = $robot->getAgent();

   Returns the agent that is being used by the robot.

addUrl
------

     $robot->addUrl( $url1, ..., $urlN );

   Used to add one or more URLs to the queue for the robot.  Each URL can
be passed as a simple string, or as a URI::URL object.

   Returns True (non-zero) if all URLs were successfully added, False
(zero) if at least one of the URLs could not be added.

unshiftUrl
----------

     $robot->unshiftUrl( $url1, ..., $urlN );

   Used to add one or more URLs to the queue for the robot.  Each URL can
be passed as a simple string, or as a URI::URL object.

   Returns True (non-zero) if all URLs were successfully added, False
(zero) if at least one of the URLs could not be added.

listUrls
--------

     $robot->listUrls( );

   Returns a list of the URLs currently in the robots list to be traversed.

addHook
-------

     $robot->addHook( $hook_name, \&hook_function );
     
     sub hook_function { ... }

   Register a *hook* function which should be invoked by the robot at a
specific point in the control flow. There are a number of *hook points* in
the robot, which are identified by a string.  For a list of hook points,
see the SUPPORTED HOOKS section below.

   If you provide more than one function for a particular hook, then the
hook functions will be invoked in the order they were added.  I.e. the
first hook function called will be the first hook function you added.

proxy, no_proxy, env_proxy
--------------------------

   These are convenience functions are setting proxy information on the
User agent being used to make the requests.

     $robot->proxy( protocol, proxy );

   Used to specify a proxy for the given scheme.  The protocol argument
can be a reference to a list of protocols.

     $robot->no_proxy(domain1, ... domainN);

   Specifies that proxies should not be used for the specified domains or
hosts.

     $robot->env_proxy();

   Load proxy settings from protocol*_proxy* environment variables:
`ftp_proxy', http_proxy, `no_proxy', etc.

ROBOT ATTRIBUTES
================

   This section lists the attributes used to configure a Robot object.
Attributes are set using the `setAttribute()' method, and queried using
the `getAttribute()' method.

   Some of the attributes must be set before you start the Robot (with the
run() method).  These are marked as *mandatory* in the list below.

NAME
     The name of the Robot.  This should be a sequence of alphanumeric
     characters, and is used to identify your Robot.  This is used to set
     the `User-Agent' field of HTTP requests, and so will appear in server
     logs.

     *mandatory*

VERSION
     The version number of your Robot.  This should be a floating point
     number, in the format *N.NNN*.

     *mandatory*

EMAIL
     A valid email address which can be used to contact the Robot's owner,
     for example by someone who wishes to complain about the behavior of
     your robot.

     *mandatory*

VERBOSE
     A boolean flag which specifies whether the Robot should display
     verbose status information as it runs.

     Default: 0 (false)

TRAVERSAL
     Specifies what traversal style should be adopted by the Robot.  Valid
     values are depth and *breadth*.

     Default: depth

IGNORE_TEXT
     Specifies whether the HTML structure passed to the invoke-on-contents
     hook function should include the textual content of the page, or just
     the HTML elements.

     Default: 1 (true)

IGNORE_UNKNOWN
     Specifies whether the HTML structure passed to the invoke-on-contents
     hook function should ignore unkonwn HTML elements.

     Default: 1 (true)

CHECK_MIME_TYPES
     This tells the robot that if it can't easily determine the MIME type
     of a link from its URL, to issue a HEAD request to check the MIME
     type directly, before adding the link.

     Default: 1 (true)

USERAGENT
     Allows the caller to specify its own user agent to make the HTTP
     requests.

     Default: LWP::RobotUA object created by the robot

ACCEPT_LANGUAGE
     Optionally allows the caller to specify the list of languages that
     the robot accepts. This is added as an "Accept-Language" header field
     in the HTTP request. Takes an array reference.

DELAY
     Optionally set the delay between requests for the user agent, in
     minutes. The default for this is 1 (see LWP::RobotUA).

SUPPORTED HOOKS
===============

   This section lists the hooks which are supported by the WWW::Robot
module.  The first two arguments passed to a hook function are always the
Robot object followed by the name of the hook being invoked. I.e. the
start of a hook function should look something like:

     sub my_hook_function
     {
         my $robot = shift;
         my $hook  = shift;
         # ... other, hook-specific, arguments

   Wherever a hook function is passed a `$url' argument, this will be a
URI::URL object, with the URL fully specified.  I.e. even if the URL was
seen in a relative link, it will be passed as an absolute URL.

restore-state
-------------

     sub hook { my($robot, $hook_name) = @_; }

   This hook is invoked just before entering the main iterative loop of
the robot.  The intention is that the hook will be used to restore state,
if such an operation is required.

   This can be helpful if the robot is running in an incremental mode,
where state is saved between each run of the robot.

invoke-on-all-url
-----------------

     sub hook { my($robot, $hook_name, $url) = @_; }

   This hook is invoked on all URLs seen by the robot, regardless of
whether the URL is actually traversed.  In addition to the standard
`$robot' and `$hook' arguments, the third argument is `$url', which is the
URL being travered by the robot.

   For a given URL, the hook function will be invoked at most once,
regardless of how many times the URL is seen by the Robot.  If you are
interested in seeing the URL every time, you can use the invoke-on-link
hook.

follow-url-test
---------------

     sub hook { my($robot, $hook_name, $url) = @_; return $boolean; }

   This hook is invoked to determine whether the robot should traverse the
given URL.  If the hook function returns 0 (zero), then the robot will do
nothing further with the URL.  If the hook function returns non-zero, then
the robot will get the contents of the URL, invoke further hooks, and
extract links if the contents are HTML.

invoke-on-followed-url
----------------------

     sub hook { my($robot, $hook_name, $url) = @_; }

   This hook is invoked on URLs which are about to be traversed by the
robot; i.e. URLs which have passed the follow-url-test hook.

invoke-on-get-error
-------------------

     sub hook { my($robot, $hook_name, $url, $response) = @_; }

   This hook is invoked if the Robot ever fails to get the contents of a
URL.  The `$response' argument is an object of type HTTP::Response.

invoke-on-contents
------------------

     sub hook { my($robot, $hook, $url, $response, $structure) = @_; }

   This hook function is invoked for all URLs for which the contents are
successfully retrieved.

   The `$url' argument is a URI::URL object for the URL currently being
processed by the Robot engine.

   The `$response' argument is an HTTP::Response object, the result of the
GET request on the URL.

   The `$structure' argument is an HTML::Element object which is the root
of a tree structure constructed from the contents of the URL.  You can set
the IGNORE_TEXT attribute to specify whether the structure passed includes
the textual content of the page, or just the HTML elements.  You can set
the IGNORE_UNKNOWN attribute to specify whether the structure passed
includes unkown HTML elements.

invoke-on-link
--------------

     sub hook { my($robot, $hook_name, $from_url, $to_url) = @_; }

   This hook function is invoked for all links seen as the robot traverses.
When the robot is parsing a page (*$from_url*) for links, for every link
seen the invoke-on-link hook is invoked with the URL of the source page,
and the destination URL.  The destination URL is in canonical form.

add-url-test
------------

     sub hook { my($robot, $hook_name, $url) = @_; }

   This hook function is invoked for all links seen as the robot traverses.
If the hook function returns non-zero, then the robot will add the URL
given by $url to its list of URLs to be traversed.

continue-test
-------------

     sub hook { my($robot) = @_; }

   This hook is invoked at the end of the robot's main iterative loop.  If
the hook function returns non-zero, then the robot will continue execution
with the next URL.  If the hook function returns zero, then the Robot will
terminate the main loop, and close down after invoking the following two
hooks.

   If no continue-test hook function is provided, then the robot will
always loop.

save-state
----------

     sub hook { my($robot) = @_; }

   This hook is used to save any state information required by the robot
application.

generate-report
---------------

     sub hook { my($robot) = @_; }

   This hook is used to generate a report for the run of the robot, if
such is desired.

modified-since
--------------

   If you provide this hook function, it will be invoked for each URL
before the robot actually requests it.  The function can return a time to
use with the If-Modified-Since HTTP header.  This can be used by a robot
to only process those pages which have changed since the last visit.

   Your hook function should be declared as follows:

     sub modifed_since_hook
     {
         my $robot = shift;        # instance of Robot module
         my $hook  = shift;        # name of hook invoked
         my $url   = shift;        # URI::URL for the url in question
     
         # ... calculate time ...
         return $time;
     }

   If your function returns anything other than undef, then a
*If-Modified-Since:* field will be added to the request header.

invoke-after-get
----------------

   This hook function is invoked immediately after the robot makes each
GET request.  This means your hook function will see every type of
response, not just successful GETs.  The hook function is passed two
arguments: the `$url' we tried to GET, and the `$response' which resulted.

   If you provided a modified-since hook, then provide an invoke-after-get
function, and look for error code 304 (or RC_NOT_MODIFIED if you are using
HTTP::Status, which you should be :-):

     sub after_get_hook
     {
         my($robot, $hook, $url, $response) = @_;
     
         if ($response->code == RC_NOT_MODIFIED)
         {
         }
     }

EXAMPLES
========

   This section illustrates use of the Robot module, with code snippets
from several sample Robot applications.  The code here is not intended to
show the right way to code a web robot, but just illustrates the API for
using the Robot.

Validating Robot
----------------

   This is a simple robot which you could use to validate your web site.
The robot uses *weblint* to check the contents of URLs of type *text/html*

     #!/usr/bin/perl
     require 5.002;
     use WWW::Robot;
     
     $rootDocument = $ARGV[0];
     
     $robot = new WWW::Robot('NAME'     =>  'Validator',
     			   'VERSION'  =>  1.000,
     			   'EMAIL'    =>  'fred@foobar.com');
     
     $robot->addHook('follow-url-test', \&follow_test);
     $robot->addHook('invoke-on-contents', \&validate_contents);
     
     $robot->run($rootDocument);
     
     #-------------------------------------------------------
     sub follow_test {
        my($robot, $hook, $url) = @_;
     
        return 0 unless $url->scheme eq 'http';
        return 0 if $url =~ /\.(gif|jpg|png|xbm|au|wav|mpg)$/;
     
        #---- we're only interested in pages on our site ----
        return $url =~ /^$rootDocument/;
     }
     
     #-------------------------------------------------------
     sub validate_contents {
        my($robot, $hook, $url, $response, $structure) = @_;
     
        return unless $response->content_type eq 'text/html';

     # some validation on $structure ...
     
        }

   If you are behind a firewall, then you will have to add something like
the following, just before calling the run() method:

     $robot->proxy(['ftp', 'http', 'wais', 'gopher'],
     		 'http://firewall:8080/');

MODULE DEPENDENCIES
===================

   The Robot.pm module builds on a lot of existing Net, WWW and other Perl
modules.  Some of the modules are part of the core Perl distribution, and
the latest versions of all modules are available from the Comprehensive
Perl Archive Network (CPAN).  The modules used are:

HTTP::Request
     This module is used to construct HTTP requests, when retrieving the
     contents of a URL, or using the HEAD request to see if a URL exists.

HTML::LinkExtor
     This is used to extract the URLs from the links on a page.

HTML::TreeBuilder
     This module builds a tree data structure from the contents of an HTML
     page.  This is also used to check for page-specific Robot exclusion
     commands, using the META element.

URI::URL
     This module implements a class for URL objects, providing resolution
     of relative URLs, and access to the different components of a URL.

LWP::RobotUA
     This is a wrapper around the LWP::UserAgent class.  A UserAgent is
     used to connect to servers over the network, and make requests.  The
     RobotUA module provides transparent compliance with the *Robot
     Exclusion Protocol*.

HTTP::Status
     This has definitions for HTTP response codes, so you can say
     RC_NOT_MODIFIED instead of 304.

   All of these modules are available as part of the libwww-perl5
distribution, which is also available from CPAN.

SEE ALSO
========

The SAS Group Home Page
     http://www.cre.canon.co.uk/sas.html

     This is the home page of the Group at Canon Research Centre Europe
     who are responsible for Robot.pm.

Robot Exclusion Protocol
     http://info.webcrawler.com/mak/projects/robots/norobots.html

     This is a *de facto* standard which defines how a `well behaved'
     Robot client should interact with web servers and web pages.

Guidelines for Robot Writers
     http://info.webcrawler.com/mak/projects/robots/guidelines.html

     Guidelines and suggestions for those who are (considering) developing
     a web robot.

Weblint Home Page
     http://www.cre.canon.co.uk/~neilb/weblint/

     Weblint is a perl script which is used to check HTML for syntax
     errors and stylistic problems, in the same way *lint* is used to
     check C.

Comprehensive Perl Archive Network (CPAN)
     http://www.perl.com/perl/CPAN/

     This is a well-organized collection of Perl resources, such as
     modules, documents, and scripts.  CPAN is mirrored at FTP sites
     around the world.

VERSION
=======

   This documentation describes version 0.021 of the Robot module.  The
module requires at least version 5.002 of Perl.

AUTHOR
======

     Neil Bowers <neilb@cre.canon.co.uk>
     Ave Wrigley <wrigley@cre.canon.co.uk>

     Web Department, Canon Research Centre Europe

COPYRIGHT
=========

   Copyright (C) 1997, Canon Research Centre Europe.

   This module is free software; you can redistribute it and/or modify it
under the same terms as Perl itself.


File: pm.info,  Node: WWW/RobotRules,  Next: WWW/RobotRules/AnyDBM_File,  Prev: WWW/Robot,  Up: Module List

Parse robots.txt files
**********************

NAME
====

   WWW::RobotsRules - Parse robots.txt files

SYNOPSIS
========

     require WWW::RobotRules;
     my $robotsrules = new WWW::RobotRules 'MOMspider/1.0';

     use LWP::Simple qw(get);

     $url = "http://some.place/robots.txt";
     my $robots_txt = get $url;
     $robotsrules->parse($url, $robots_txt);

     $url = "http://some.other.place/robots.txt";
     my $robots_txt = get $url;
     $robotsrules->parse($url, $robots_txt);

     # Now we are able to check if a URL is valid for those servers that
     # we have obtained and parsed "robots.txt" files for.
     if($robotsrules->allowed($url)) {
         $c = get $url;
         ...
     }

DESCRIPTION
===========

   This module parses a `/robots.txt' file as specified in "A Standard for
Robot Exclusion", described in
<http://info.webcrawler.com/mak/projects/robots/norobots.html> Webmasters
can use the `/robots.txt' file to disallow conforming robots access to
parts of their web site.

   The parsed file is kept in the WWW::RobotRules object, and this object
provides methods to check if access to a given URL is prohibited.  The
same WWW::RobotRules object can parse multiple `/robots.txt' files.

   The following methods are provided:

$rules = WWW::RobotRules->new($robot_name)
     This is the constructor for WWW::RobotRules objects.  The first
     argument given to new() is the name of the robot.

$rules->parse($robot_txt_url, $content, $fresh_until)
     The parse() method takes as arguments the URL that was used to
     retrieve the `/robots.txt' file, and the contents of the file.

$rules->allowed($uri)
     Returns TRUE if this robot is allowed to retrieve this URL.

$rules->agent([$name])
     Get/set the agent name. NOTE: Changing the agent name will clear the
     robots.txt rules and expire times out of the cache.

ROBOTS.TXT
==========

   The format and semantics of the "/robots.txt" file are as follows (this
is an edited abstract of
<http://info.webcrawler.com/mak/projects/robots/norobots.html>):

   The file consists of one or more records separated by one or more blank
lines. Each record contains lines of the form

     <field-name>: <value>

   The field name is case insensitive.  Text after the '#' character on a
line is ignored during parsing.  This is used for comments.  The following
<field-names> can be used:

User-Agent
     The value of this field is the name of the robot the record is
     describing access policy for.  If more than one *User-Agent* field is
     present the record describes an identical access policy for more than
     one robot. At least one field needs to be present per record.  If the
     value is '*', the record describes the default access policy for any
     robot that has not not matched any of the other records.

Disallow
     The value of this field specifies a partial URL that is not to be
     visited. This can be a full path, or a partial path; any URL that
     starts with this value will not be retrieved

ROBOTS.TXT EXAMPLES
===================

   The following example "/robots.txt" file specifies that no robots
should visit any URL starting with "/cyberworld/map/" or "/tmp/":

     User-agent: *
     Disallow: /cyberworld/map/ # This is an infinite virtual URL space
     Disallow: /tmp/ # these will soon disappear

   This example "/robots.txt" file specifies that no robots should visit
any URL starting with "/cyberworld/map/", except the robot called
"cybermapper":

     User-agent: *
     Disallow: /cyberworld/map/ # This is an infinite virtual URL space

     # Cybermapper knows where to go.
     User-agent: cybermapper
     Disallow:

   This example indicates that no robots should visit this site further:

     # go away
     User-agent: *
     Disallow: /

SEE ALSO
========

   *Note LWP/RobotUA: LWP/RobotUA,, *Note WWW/RobotRules/AnyDBM_File:
WWW/RobotRules/AnyDBM_File,


File: pm.info,  Node: WWW/RobotRules/AnyDBM_File,  Next: WWW/Search,  Prev: WWW/RobotRules,  Up: Module List

Persistent RobotRules
*********************

NAME
====

   WWW::RobotRules::AnyDBM_File - Persistent RobotRules

SYNOPSIS
========

     require WWW::RobotRules::AnyDBM_File;
     require LWP::RobotUA;

     # Create a robot useragent that uses a diskcaching RobotRules
     my $rules = new WWW::RobotRules::AnyDBM_File 'my-robot/1.0', 'cachefile';
     my $ua = new WWW::RobotUA 'my-robot/1.0', 'me@foo.com', $rules;

     # Then just use $ua as usual
     $res = $ua->request($req);

DESCRIPTION
===========

   This is a subclass of *WWW::RobotRules* that uses the AnyDBM_File
package to implement persistent diskcaching of `robots.txt' and host visit
information.

   The constructor (the new() method) takes an extra argument specifying
the name of the DBM file to use.  If the DBM file already exists, then you
can specify undef as agent name as the name can be obtained from the DBM
database.

SEE ALSO
========

   *Note WWW/RobotRules: WWW/RobotRules,, *Note LWP/RobotUA: LWP/RobotUA,

AUTHORS
=======

   Hakan Ardo <hakan@munin.ub2.lu.se>, Gisle Aas <aas@sn.no>


File: pm.info,  Node: WWW/Search,  Next: WWW/Search/AOL/Classifieds/Employment,  Prev: WWW/RobotRules/AnyDBM_File,  Up: Module List

Virtual base class for WWW searches
***********************************

NAME
====

   WWW::Search - Virtual base class for WWW searches

SYNOPSIS
========

     require WWW::Search;
     $search_engine = "AltaVista";
     $search = new WWW::Search($search_engine);

DESCRIPTION
===========

   This class is the parent for all access methods supported by the
`WWW::Search' library.  This library implements a Perl API to web-based
search engines.

   See README for a list of search engines currently supported.

   Search results can be limited, and there is a pause between each
request to avoid overloading either the client or the server.

Sample program
--------------

   Using the library should be straightforward.  Here is a sample program:

     my $search = new WWW::Search('AltaVista');
     $search->native_query(WWW::Search::escape_query($query));
     while (my $result = $search->next_result())
       {
       print $result->url, "\n";
       }

   Results are objects of type `WWW::SearchResult' (see *Note
WWW/SearchResult: WWW/SearchResult, for details).  Note that different
backends support different result fields.  All backends are required to
support title and url.

SEE ALSO
========

   For specific search engines, see `WWW::Search::TheEngineName' in this
node (replacing TheEngineName with a particular search engine).

   For details about the results of a search, see *Note WWW/SearchResult:
WWW/SearchResult,.

METHODS AND FUNCTIONS
=====================

new
---

   To create a new WWW::Search, call

     $search = new WWW::Search('SearchEngineName');

   where SearchEngineName is replaced with a particular search engine.
For example:

     $search = new WWW::Search('Google');

   If no search engine is specified a default (currently 'AltaVista') will
be chosen for you.  The next step is usually:

     $search->native_query('search-engine-specific+query+string');

reset_search (PRIVATE)
----------------------

   Resets internal data structures to start over with a new search.

version
-------

   Returns the value of the $VERSION variable of the backend engine, or
$WWW::Search::VERSION if the backend does not contain $VERSION.

maintainer
----------

   Returns the value of the $MAINTAINER variable of the backend engine, or
$WWW::Search::MAINTAINER if the backend does not contain $MAINTAINER.

gui_query
---------

   Specify a query to the current search object; the query will be
performed with the engine's default options, as if it were typed by a user
in a browser window.

   The query must be escaped; call `&WWW::Search::escape_query' in this
node to escape a plain query.  See native_query below for more information.

   Currently, this feature is supported by only a few backends; consult
the documentation for each backend to see if it is implemented.

native_query
------------

   Specify a query (and optional options) to the current search object.
Previous query (if any) and its cached results (if any) will be thrown
away.  The option values and the query must be escaped; call
`WWW::Search::escape_query()' in this node to escape a string.  The search
process is not actually begun until results or next_result is called
(lazy!), so native_query does not return anything.

   Example:

     $search->native_query('search-engine-specific+escaped+query+string',
                           { option1 => 'able', option2 => 'baker' } );

   The hash of options following the query string is optional.  The query
string is backend-specific.  There are two kinds of options: options
specific to the backend, and generic options applicable to multiple
backends.

   Generic options all begin with 'search_'.  Currently a few are
supported:

search_url
     Specifies the base URL for the search engine.

search_debug
     Enables backend debugging.  The default is 0 (no debugging).

search_parse_debug
     Enables backend parser debugging.  The default is 0 (no debugging).

search_method
     Specifies the HTTP method (GET or POST) for HTTP-based queries.  The
     default is GET

search_to_file FILE
     Causes the search results to be saved in a set of files prefixed by
     FILE.  (Used internally by the test-suite, not intended for general
     use.)

search_from_file FILE
     Reads a search from a set of files prefixed by FILE.  (Used
     internally by the test-suite, not intended for general use.)

   Some backends may not implement these generic options, but any which do
implement them must provide these semantics.

   Backend-specific options are described in the documentation for each
backend.  In most cases the options and their values are packed together
to create the query portion of the final URL.

   Details about how the search string and option hash are interpreted
might be found in the search-engine-specific manual pages
(WWW::Search::SearchEngineName).

   After native_query, the next step is usually:

     while ($result = $search->next_result())
       {
       # do_something;
       }

cookie_jar
----------

   Call this method (anytime before asking for results) if you want to
communicate cookie data with the search engine.  Takes one argument,
either a filename or an HTTP::Cookies object.  If you give a filename,
WWW::Search will attempt to read/store cookies there (by in turn passing
the filename to HTTP::Cookies::new).

     $oSearch->cookie_jar('/tmp/my_cookies');

   If you give an HTTP::Cookies object, it is up to you to save the
cookies if you wish.

     use HTTP::Cookies;
     my $oJar = HTTP::Cookies->new(...);
     $oSearch->cookie_jar($oJar);

approximate_result_count
------------------------

   Some backends indicate how many hits they have found.  Typically this
is an approximate value.

results
-------

   Return all the results of a query as a reference to array of
SearchResult objects.  Example:

     @results = $search->results();
     foreach $result (@results) {
         print $result->url(), "\n";
     };

   On error, results() will return undef and set response() to the HTTP
response code.

next_result
-----------

   Call this method repeatedly to return each result of a query as a
SearchResult object.  Example:

     while ($result = $search->next_result())
       {
       print $result->url(), "\n";
       }

   On error, next_result() will return undef and set response() to the
HTTP response code.

response
--------

   Return the HTTP Response code for the last query (see *Note
HTTP/Response: HTTP/Response,).  If the query returns undef, errors could
be reported like this:

     my $response = $search->response();
     if ($response->is_success) {
     	print "normal end of result list\n";
     } else {
     	print "error:  " . $response->as_string() . "\n";
     }

   Note: even if the backend does not involve the web, it should return
HTTP::Response-style codes.

seek_result($offset)
--------------------

   Set which result next_result should return (like lseek in Unix).
Results are zero-indexed.

   The only guaranteed valid offset is 0, which will replay the results
from the beginning.  In particular, seeking past the end of the current
cached results probably will not do what you might think it should.

   Results are cached, so this does not re-issue the query or cause IO
(unless you go off the end of the results).  To re-do the query, create a
new search object.

   Example:

     $search->seek_result(0);

maximum_to_retrieve
-------------------

   The maximum number of hits to return.  Queries resulting in more than
this many hits will return the first hits, up to this limit.  Although
this specifies a maximum limit, search engines might return less than this
number.

   Defaults to 500.

   Example:     $max = $search->maximum_to_retrieve(100);

timeout
-------

   The maximum length of time any portion of the query should take, in
seconds.

   Defaults to 60.

   Example:     $search->timeout(120);

opaque
------

   This function provides an application a place to store one opaque data
element (or many via a Perl reference).  This facility is useful to (for
example), maintain client-specific information in each active query when
you have multiple concurrent queries.

escape_query
------------

   Escape a query.  Before queries are made special characters must be
escaped so that a proper URL can be formed.  This is like escaping a URL,
but all non-alphanumeric characters are escaped and and spaces are
converted to "+"s.

   Example:     $escaped = WWW::Search::escape_query('+hi +mom');

     (Returns "%2Bhi+%2Bmom").

   See also unescape_query.  NOTE that this is not a method, it is a plain
function.

unescape_query
--------------

   Unescape a query.  See escape_query for details.

   Example:     $unescaped = WWW::Search::unescape_query('%22hi+mom%22');

     (Returns '"hi mom"').

   NOTE that this is not a method, it is a plain function.

strip_tags
----------

   Given a string, returns a copy of that string with HTML tags removed.
This should be used by each backend as they insert the title and
description values into the SearchResults.

hash_to_cgi_string (PRIVATE) (DEPRECATED)
-----------------------------------------

   Deprecated.

   Given a reference to a hash of string => string, constructs a CGI
parameter string that looks like 'key1=value1&key2=value2'.

   At one time, for testing purposes, we asked backends to use this
function rather than piecing the URL together by hand, to ensure that URLs
are identical across platforms and software versions.  But this is no
longer necessary.

   Example:

     $self->{_options} = {
                          'opt3' => 'val3',
                          'search_url' => 'http://www.deja.com/dnquery.xp',
                          'opt1' => 'val1',
                          'QRY' => $native_query,
                          'opt2' => 'val2',
                         };
     $self->{_next_url} = $self->{_options}{'search_url'} .'?'.
                          $self->hash_to_cgi_string($self->{_options});

http_proxy
----------

   Set-up an HTTP proxy (for connections from behind a firewall).

   This routine should be called before the first retrieval is attempted.

   Example:

     $search->http_proxy("http://gateway:8080");

user_agent($NON_ROBOT) (PRIVATE)
--------------------------------

   This internal routine creates a user-agent for derived classes that
query the web.  If non-empty argument $non_robot is given, a normal
user-agent (rather than a robot-style user-agent) is used.

   If a backend needs the low-level LWP::UserAgent or LWP::RobotUA to have
a particular name, $oSearch->{'agent_name'} (and possibly
$oSearch->{'agent_e_mail'}) should be set to the desired values before
calling $oSearch->user_agent():

     $oSearch = new WWW::Search('NewBackend');
     $oSearch->{'agent_e_mail'} = $oSearch->{'agent_name'};
     $oSearch->{'agent_name'} = 'Mozilla/5.5';
     $oSearch->user_agent('non-robot');

   Backends should use robot-style user-agents whenever possible.  Also,
backends should call `user_agent_delay' between every page retrieval to
avoid swamping search-engines.

http_request($method, $url)
---------------------------

   Return the response from an http request, handling debugging.  Requires
that user_agent already be set up.  For POST methods, query is split off
of the URL and passed in the request body.

split_lines (PRIVATE)
---------------------

   This internal routine splits data (typically the result of the web page
retrieval) into lines in a way that is OS independent.  If the first
argument is a reference to an array, that array is taken to be a list of
possible delimiters for this split.  For example, Yahoo.pm uses <p> and
<dd><li> as "line" delimiters for convenience.

generic_option (PRIVATE)
------------------------

   This internal routine checks if an option is generic or backend
specific.  Currently all generic options begin with 'search_'.  This
routine is not a method.

setup_search (PRIVATE)
----------------------

   This internal routine does generic Search setup.  It calls
`native_setup_search' to do backend specific setup.

user_agent_delay (PRIVATE)
--------------------------

   Derived classes should call this between requests to remote servers to
avoid overloading them with many, fast back-to-back requests.

absurl (PRIVATE)
----------------

   An internal routine to convert a relative URL into a absolute URL.  It
takes two arguments, the 'base' url (usually the search engine CGI URL)
and the URL to be converted.  Returns a URI::URL object.

retrieve_some (PRIVATE)
-----------------------

   An internal routine to interface with `native_retrieve_some'.  Checks
for overflow.

test_cases
----------

   Returns the value of the $TEST_CASES variable of the backend engine.
All backends should set $TEST_CASES to a string containing perl code which
will be eval-ed during 'make test'.  See Excite.pm for an example.

IMPLEMENTING NEW BACKENDS
=========================

   `WWW::Search' supports backends to separate search engines.  Each
backend is implemented as a subclass of `WWW::Search'.  *Note
WWW/Search/AltaVista: WWW/Search/AltaVista, provides a good sample backend.

   A backend must have the two routines `native_retrieve_some' and
`native_setup_search'.

   `native_retrieve_some' is the core of a backend.  It will be called
periodically to fetch URLs.  It should retrieve several hits from the
search service and add them to the cache.  It should return the number of
hits found, or undef when there are no more hits.

   Internally, `native_retrieve_some' typically sends an HTTP request to
the search service, parse the HTML, extract the links and descriptions,
then save the URL for the next page of results.  See the code for the
AltaVista implementation for an example.

   `native_setup_search' is invoked before the search.  It is passed a
single argument:  the escaped, native version of the query.

   The front- and backends share a single object (a hash).  The backend
can change any hash element beginning with underscore, and `{response}'
(an `HTTP::Response' code) and `{cache}' (the array of `WWW::SearchResult'
objects caching all results).  Again, look at one of the existing web
search backends as an example.

   If you implement a new backend, please let the authors know.

BUGS AND DESIRED FEATURES
=========================

   The bugs are there for you to find (some people call them Easter Eggs).

   Desired features:

A portable query language.
     A portable language would easily allow you to move queries easily
     between different search engines.  A query abstraction is non-trivial
     and unfortunately will not be done anytime soon by the current
     maintainers.  If you want to take a shot at it, please let me know.

AUTHOR
======

   `WWW::Search' was written by John Heidemann, <johnh@isi.edu>.
`WWW::Search' is currently maintained by Martin Thurn,
<MartinThurn@iname.com>.

   backends and applications for WWW::Search were originally written by
John Heidemann, Wm. L. Scheding, Cesare Feroldi de Rosa, and GLen Pringle.

COPYRIGHT
=========

   Copyright (c) 1996 University of Southern California.  All rights
reserved.

   Redistribution and use in source and binary forms are permitted
provided that the above copyright notice and this paragraph are duplicated
in all such forms and that any documentation, advertising materials, and
other materials related to such distribution and use acknowledge that the
software was developed by the University of Southern California,
Information Sciences Institute.  The name of the University may not be
used to endorse or promote products derived from this software without
specific prior written permission.

   THIS SOFTWARE IS PROVIDED "AS IS" AND WITHOUT ANY EXPRESS OR IMPLIED
WARRANTIES, INCLUDING, WITHOUT LIMITATION, THE IMPLIED WARRANTIES OF
MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE.


